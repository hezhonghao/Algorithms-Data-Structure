{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Oct 18 - Work in progress <br>\n",
        "References:<br>\n",
        "https://int8.io/monte-carlo-tree-search-beginners-guide/<br>\n",
        "https://www.geeksforgeeks.org/ml-monte-carlo-tree-search-mcts/<br>\n",
        "https://www.reddit.com/r/singularity/comments/1cmcmy5/monte_monte_carlo_tree_search_with_llms_is_the/ <br>\n",
        "Search: Games, Minimax, and Alpha-Beta: https://ocw.mit.edu/courses/6-034-artificial-intelligence-fall-2010/resources/lecture-6-search-games-minimax-and-alpha-beta/\n",
        "https://courses.cs.washington.edu/courses/cse599i/18wi/resources/lecture19/lecture19.pdf<br>\n",
        "applications illustrated: https://www.informs-sim.org/wsc18papers/includes/files/021.pdf<br>\n",
        "tic-tak-toe: https://stackoverflow.com/questions/49456415/monte-carlo-tree-search-tic-tac-toe-poor-agent <br>\n",
        "\n",
        "some leetcode adaptations: https://www.perplexity.ai/search/any-problems-at-leetcode-that-OgI491GFTbKDPRnDknJCUg\n"
      ],
      "metadata": {
        "id": "yLGOO2DqyQDp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MCTS\n",
        "## What is it for?\n",
        "- To create a tree structure search algorithm that balances depth and breadth (exploitation and exploration)\n",
        "\n",
        "\n",
        "## How it works?\n",
        "- When traveling to each node & staying at certain board state, you simulate the expected returns if traveling a given path to the end, and return rewards to each node at that path.\n",
        "- You should probably estimate current resources, too.\n",
        "- The resources and rewards may or may not be the same thing. Likely you may gain resources as rewards, but very likely they are different things.\n",
        "- Return: MCTS is like an assistant. It takes in current board state and returns you a recommendation for the next move, according to many times of simulations (Not as you thought that it would give you a whole path).\n",
        "\n",
        "### Selection:\n",
        "\n",
        "### Expansion:\n",
        "The action of exploring untried moves and adding them to your list of children. Untried moves are those empty positions on the board.\n",
        "\n",
        "*For MCTS, you always expand first untile there is no node to be added to your children and then you select best one as the next move. So expand --> select.\n",
        "\n",
        "*expansion VS selection: two different moves, connecting new nodes VS selecting your best child;\n",
        "*exploration VS exploitation: two different strategies, exploring new nodes VS go down the current path.\n",
        "\n",
        "### Simulation:\n",
        "For a chosen node, you traverse all the way till the end to reap all the rewards along the way.\n",
        "\n",
        "What is one round of simulation? Does it do the same as one run of MCTS?\n",
        "No.\n",
        "- One round of simulation starts with a actual board state to a simulated board state where you receive an end state (no further move can be made)\n",
        "- In such a simulation, each node in the current copy of board, will connect parent and children, will go down a particular path (simulation step)\n",
        "- For each iteration of MCTS (a copy board state):\n",
        "  - selection: you select the best child node for current parent, if the current node is fully expanded and it's not terminal, yet.\n",
        "  - expansion you expand a new node as long as there is still child to be added.\n",
        "  - simulation: you start with the newly expanded node and simulate current path to the end as long as (while) there is untried nove left and no winner declared\n",
        "  - backpropagation: you return the stats of this simulation to the nodes traversed in this simulation.\n",
        "\n",
        "Key things to be differentiated between selection, expansion, and simulation:\n",
        "- Selection is to exploit and expansion to explore.\n",
        "- Expansion connects one new node a time, and simulation continues that new node to the end of this path.\n",
        "- Selection also down the path of selected node, but with each move calculated whereas simulation does things randomly. You'd hope this randomization to give you a bit signal (by BP) that you can used for future moves.\n",
        "\n",
        "### Backpropagation:\n",
        "return the stats (steps/resources + rewards) to parent nodes such that at parent nodes they may decide an optimal next move (for selection). It was part of, however the opposite route of the simulation. Simulation goes down the rabbithole, and GP goes out of it and back.\n",
        "\n",
        "\n",
        "### Overall MCTS workflow\n",
        "- I had some fundamental misunderstanding of its workflow originally:\n",
        "  - I thougt it was a class. It probably does not work that way because a class is for algorithms + data structure where you store something. But a MCTS is just an algorithm, a stragegy for your game. It does not hold data like you would need for a board or a node.\n",
        "  - I thought you'd define 4 functions for 4 steps. It was not wrong, but just not necessary. It's perfectly fine without them.\n",
        "  - I could not differentiate MCTS workflow from what's in node/board. What's about the rules and data structure of the game should be in either node or board, whereas MCTS is about the strategy.\n",
        "  - I did not realize with a MCTS you input current board state and outut a recommended move. In retrospect, this is actually most intuitive.\n",
        "  - I did not know that you create copies of board states on which you would run 4 steps (not only simulation!).\n",
        "  - I did not know that any changes on references (associated to each iteration/copy of board state) to the root nodes (what you are assigned when starting the MCTS) will also apply to the root nodes. Like the stats (wins/visits), and children nodes connected to.\n",
        "\n",
        "- Tips for connecting diff class/functions together\n",
        "  -\n",
        "\n",
        "## Board and Node\n",
        "- Node: Each node is a TreeNode, where you have attributes and methods can be applied to each node. Please be reminded that everything you'd apply to a node, like a list, is independent from a different node.\n",
        "- Board: where you set up result, board state,\n",
        "- The tricky part for this game is though, you have to get a copy of _current board state_ for each simulation, the real deep copy that does not affect one another.\n",
        "- Connections between Node and Board: When you make a move, your node changes (you select next move based on UCB1) and your board changes, too (the current board state).\n",
        "- Mind that SELECT and EXPAND are very different operations. SELECT makes next move, where EXPAND adds a potential child to current parent thereafter you may do simulation with.\n",
        "- UCB1: an algorithm, or actually just one sorting funciton that to balence exploration and exploitation. Two terms: exploitation term that to calculate wins/visits of current path; and exploration term that to encourage exploring different paths where you've visited less and are less certain about their wins/visits.\n",
        "\n",
        "What is really a board? Is that a static board state, as you've regarded as, or a dynamic one?\n",
        "If a dynamic one, where to draw the boundary (what is a board, in contrast to other boards?)\n",
        "- I think a board should represent a static board state, with moves actually made, not simulated.\n",
        "- In contrast, a copy represents a board state that starts with the actual one, but has speculated moves on there.\n",
        "- so when you define a board state, you have # of nodes defined, each represent a actual moved position on there.\n",
        "\n",
        "What is really a node if it does not represent a move (bc MCTS returns a move)\n",
        "- I think a node represent a position, travelled or untravelled (but extended, and then simulated), belongs to a give board state. <br>\n",
        "- The above understanding is not complete. I think you had this conception that a node is a defined position on the board state. This is only true when you define a node with `node = Node(idx = idx)`. But a node can work perfect fine if `idx=None`. So a more nuanced understanding of a node is it's a position on current board state but it's idx can be unknown. A node would be just a node structure that you can have all its nice attributes and methods. In this TTT example, you may connect a node to its parent & children (both speak for its relative positions), you give it an idx, a board state with `(0,1;x,o)` all indictaed. However, any of them can be set as `None`.\n",
        "  - So yes it should be good to have idx of a node, but it's fine if not. And its idx can be either user-identified, or randomly choosen in `EXPAND`. <br>\n",
        "\n",
        "What is a fully expanded node?\n",
        "- a fully expanded node means all untravelled nodes are added to it as children nodes and there is no need to further expand. We can now explore/select best node out of all children\n",
        "\n",
        "Is a node configurated differently in different copies and different board states?\n",
        "- visit & win counts are accumulated through many rounds if simulations.\n",
        "- for this reason (visit & win counts), I think node is defined independent from board states/copy states. There should be 9 nodes defined, period.\n",
        "- However, in different simulations (and by extension, different copies), node may be modified by which parent/children they are connected, too.\n",
        "- In a pre-defined/user-defined board state, node positions are defined, but node connections are not. They are just on board states and you do not know how do they reach that state.\n",
        "- When you start a new copy of current board state, self.children, a.k.a node connection configuration, should be empty.\n",
        "\n",
        "## TreeNode data structure\n",
        "- just like link\n",
        "\n",
        "## Game/RL\n",
        "- Design of reward function: yes it should be updated back to every single node, but the calculation is at the final result, which is why it's called \"backpropogation\". I am writting this because I misunderstood reward function in TTT game is reward = 1 2 3 when you get 1 2 3 in a line. It does not work that way. It should be calculated when you get a 3 in simulation, and then you make the way back to the nodes.\n",
        "- WHen do you need a reward function?\n",
        "  - When the steps are more than 2 (otherwise you do move?), and the space for exploration is large and you will need to make simulation and allocate your rewards to those winning strategies in the simulation.\n",
        "- Policy: I think it's wrong to have a policy up here. Beacuse Node is to deal with attributes and actions on node. I know you wanted it to move \"according to certain policy\", but it actually just basic \"move\". Your intended policy should go with MCTS.\n",
        "\n",
        "\n",
        "\n",
        "## Why it may work better or worse than other methods in diff areas?\n",
        "- MCTS should work better in scenarios where you cannot traverse all pathes (potentially your space of moves is enormously large and even infinite). Here MCTS's random search may guide the decisions between exploration and exploitation.\n",
        "\n",
        "## Where things may get tricky?\n",
        "- It seems MCTS is computationally demanding\n",
        "-\n",
        "\n",
        "Why does it even work in many applications?\n",
        "-\n",
        "- Computation would not be a limitation for employijng such strategy in reality. In other words, simulation and computation enable calculaton of best strategy. <br><br><br>\n",
        "\n",
        "\n",
        "The main concept of monte carlo tree search is a search. Search is a set of traversals down the game tree. Single traversal is a path from a root node (current game state) to a node that is not fully expanded. Node being not-fully expanded means at least one of its children is unvisited, not explored. Once not fully expanded node is encountered, one of its unvisited children is chosen to become a root node for a single playout/simulation. The result of the simulation is then propagated back up to the current tree root updating game tree nodes statistics. Once the search (constrained by time or computational power) terminates, the move is chosen based on the gathered statistics.\n",
        "\n",
        "Lots of dots not clearly connected together, right?\n",
        "\n",
        "Let’s try to ask crucial questions regarding the simplified description above to slowly understand all the pieces:\n",
        "\n",
        "what are expanded or not fully unexpanded game tree nodes?\n",
        "what it means ‘to traverse down‘ during search? How is the next (child) node selected?\n",
        "what is a simulation?\n",
        "what is the backpropagation?\n",
        "what statistics are back-propagated and updated in expanded game tree nodes ?\n",
        "How is the final move even chosen?"
      ],
      "metadata": {
        "id": "Rz3MGzzootrP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7mVNk2-lqhi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "collapsed": true,
        "outputId": "7c940c89-4fff-4583-d856-e7c6d69d7f21"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "expected ':' (<ipython-input-1-6566e6b25141>, line 12)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-6566e6b25141>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    def forward(self, resources, node)\u001b[0m\n\u001b[0m                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected ':'\n"
          ]
        }
      ],
      "source": [
        "# Pseudocode for MCTS (not using a class bc self. not clean )\n",
        "resources = None\n",
        "rewards = dict(nodes:rewards) # dict where a reward is associated to a specific node\n",
        "\n",
        "# overall function\n",
        "def forward(resources, node):\n",
        "  while resources:\n",
        "\n",
        "\n",
        "# Step 1 Selection: the process of deciding next move(node) based on expected rewards\n",
        "def selection(nodes, rewards):   # I expect nodes to be a tree structure but rewards to be a dict: node:reward\n",
        "  best_child = best(nodes)  # basically a for loop to go through all nodes and rewards I think\n",
        "  return best_child\n",
        "\n",
        "# Step 2 Expansion: the action of creating a node/making a move\n",
        "def expansion(nodes):\n",
        "  if resources == 0:\n",
        "    return None\n",
        "  cur.next = best_child\n",
        "  cur = cur.next\n",
        "  return cur # # NEP I could not really differentiate this from the selection, except moving the node.\n",
        "\n",
        "# Step 3 Simulation: to calculating expected rewards if you were to exploit current node/path\n",
        "def simulation(node, rewards):  # Assuming that this is to cal expected return of the current path if you exploit it.\n",
        "  simulated_rewards = num_nodes_in_simulation * averate_rewards_of_the_path\n",
        "  return simulated_rewards\n",
        "\n",
        "# Step 4 Backpropagation: return the stats to the parent node (how many nodes and how much expected rewards)\n",
        "def backpropagation():\n",
        "  return stats # signals back to the parents  dict(node:reward)\n",
        "\n",
        "# Helper functions\n",
        "\n",
        "# Strategies for expansion\n",
        "def strategies(nodes):\n",
        "  # default   # we start with a random strategy\n",
        "  pick_randome(node.children)\n",
        "\n",
        "  # best strategy (???)\n",
        "  reward_new_path = simulation(path, reward)\n",
        "  reward_best_path = list(self.best_path.values).sum()\n",
        "  if ward_new_path > reward_best_path:\n",
        "    self.best_path = new_path # where you got this new path?\n",
        "  return best_path\n",
        "\n",
        "# Monitoring steps and rewards\n",
        "\n",
        "# Monitoring resources\n",
        "def resource_monitor():   # it's called every time the agent makes a move.\n",
        "  if resources ==0:\n",
        "    return \"Game Over\"\n",
        "  resource -= 1   # there is no return so far, but when you call this global variable it always tells you current remain resources.\n",
        "\n",
        "# Monitoring rewards\n",
        "def rewards monitor():\n",
        "\n",
        "  # current rewards\n",
        "  dict(node:rewards)\n",
        "\n",
        "  # expecter rewards based on simulations\n",
        "  dict(node: expected_rewards)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A pseudocode from https://int8.io/monte-carlo-tree-search-beginners-guide/, with annotation\n",
        "# Overall function: selection - expansion - simulation - backpropagation - selection, until a optimal path is found.\n",
        "def monte_carlo_tree_search(root):\n",
        "    while resources_left(time, computational_power):  # Continue the process until resources (time or computational power) are exhausted\n",
        "        leaf = traverse(root)  # Step 1: Selection - traverse the tree to select a leaf node (unexplored node)\n",
        "        simulation_result = rollout(leaf)  # Step 2: Simulation - simulate a random play from the leaf node to a terminal state\n",
        "        backpropagate(leaf, simulation_result)  # Step 3: Backpropagation - update the stats of all the nodes from the leaf back to the root\n",
        "    return best_child(root)  # Step 4: Return the best child (the most promising move) of the root based on the number of visits or score\n",
        "    # NEP is best_child(root) a path or a node\n",
        "# Step 1: Selection - traverse the tree to select a leaf node (unexplored node)\n",
        "def traverse(node):\n",
        "    while fully_expanded(node):  # Keep traversing the tree as long as the node is fully expanded (all children have been visited)\n",
        "        node = best_ucb(node)  # Choose the child with the best UCT (Upper Confidence Bound applied to Trees) score for exploration\n",
        "    return pick_unvisited(node.children) or node  # If there are unvisited children, return one. Otherwise, return the current node (could be terminal)\n",
        "    #NEP what is UCT and do we need to write a UCT function in real code?  # NEP Do we have a function to monitor unvisited node?\n",
        "\n",
        "# Step 2: Expansion (make a move in real games)\n",
        "def expand(node):\n",
        "    if not fully_expanded(node):\n",
        "        return pick_unvisited(node.children)  # Expand by adding a new unvisited node\n",
        "    return node  # If already fully expanded, return the current node\n",
        "\n",
        "# Step 3: Simulation - simulate a random game (play) from the node until a terminal state is reached\n",
        "def rollout(node):\n",
        "    while non_terminal(node):  # Keep going until you reach a terminal node (end of the game)\n",
        "        node = rollout_policy(node)  # Apply a rollout policy (typically random) to pick the next move/child\n",
        "    return result(node)  # Return the result of the game (win, loss, or draw)\n",
        "\n",
        "# strategy used in simulation\n",
        "def rollout_policy(node):  # A simple strategy for random simulation, picks a random child (move) from the current node\n",
        "    return pick_random(node.children)\n",
        "\n",
        "# Step 3: Backpropagation - propagate the result of the simulation back up the tree\n",
        "def backpropagate(node, result):\n",
        "   if is_root(node):  # Stop when we reach the root of the tree\n",
        "       return\n",
        "   node.stats = update_stats(node, result)  # Update the node's stats (e.g., number of visits, win/loss score) based on the result\n",
        "   backpropagate(node.parent)  # Recursively backpropagate the result to the parent node\n",
        "   # NEP are results & stats the same thing?\n",
        "def best_child(node):  # Step 4: Selection - choose the child with the best stats (e.g., highest visit count or average win rate)\n",
        "    return pick_child_with_highest_visits(node.children)  # Usually, this is based on the number of visits or average reward (score)\n",
        "    # NEP I didn't cound whether you would decide best child according to visit count."
      ],
      "metadata": {
        "id": "8haO3Yc6f_Uq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Monte Carlo Tree Search Tic-Tac-Toe\n",
        "\n",
        "Housekeeping\n",
        "\n",
        "\n",
        "Steps\n",
        "1/ A board class to reflect the current state of the board, attributes and potential actions you can take on the boards.\n",
        "  - input: actions on the nodes (changes on board state thereafter)\n",
        "  - attributes: np array for current board states [2,3,3]\n",
        "  - actions: change states, return results,\n",
        "  - output: board states\n",
        "\n",
        "2/ A node class to track the agents: what set of actions each has, how much rewards and resources they would have after taking certain actions.\n",
        "  - inputs:\n",
        "  - actions: move,\n",
        "  - attributes: board states, pathes, rewards, resources,\n",
        "  - output: final pathes\n",
        "\n",
        "3/ MCTS function/class that implement the 4 steps of MCTS to execute TTT. Unclear about function/class for now. The reson that this is seperate from board/node is surely you can have different strategies for the same game. MCTS is only one of them.\n",
        "\n",
        "\n",
        "Notice:\n",
        "\n"
      ],
      "metadata": {
        "id": "PmBDguII8JNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Board class\n",
        "class Board(): # My general assumption is the states and constraints of the game come here.\n",
        "  def __init__(self, states = np.zeros((2,3,3))):  # NEP When you DIY a board state, as you will with rootstate, you will need to identify moves made by \"1\"s. By default it's supposed to be empty board.\n",
        "    self.states = states # both agents, each 3*3 boards,1s and 0s\n",
        "    self.player = 0 # it starts with one player and this variable can be toggled to the other.\n",
        "    self.reward = None  # NEP I felt uncertain about defining thing here or in the __init__ header.\n",
        "    self.untravelled = np.argwhere(self.states[0]+self.states[1]==0).tolist()  # Here we define a list of indices of nodes untravelled, meaning there is no node in that position.\n",
        "    # also constraints that reflect the rules of the game (max num of moves, conditions that decide the ends of the games, rules for each move)\n",
        "    assert self.states.sum() <= 9 # NEP to be deleted perhaps\n",
        "\n",
        "  # copy of *current* board states\n",
        "  def coping(self, states):\n",
        "    copy = np.copy(self.states)\n",
        "    copy.states  = self.states # NEP to be updated. How does it reflect current states instead of the initial one.\n",
        "    copy.player = self.player\n",
        "\n",
        "  # board_state updating\n",
        "  def board_updating(self, states, player, position):  # position: (x,y) represents position to be updated\n",
        "    while not self.reward:  # If you received a reward (True) of current board state, then you should not update the board anymore because game over.\n",
        "      raw,col = position\n",
        "      if np.argwhere(self.states[0][raw][col] + self.states[1][raw][col]==1): # occupied\n",
        "        return\n",
        "      self.states[player][raw][col] = 1\n",
        "      self.player ^= 1\n",
        "\n",
        "  # get remaining moves (unoccupied) on the board\n",
        "  def get_moves(self,):\n",
        "    if self.resulting(player):  # We will not enter the function if result is declared.\n",
        "      return\n",
        "    return np.argwhere(self.states[0][raw][col] + self.states[1][raw][col]==1)\n",
        "\n",
        "\n",
        "  def resulting (self, player): # function for returning the results (win/lose/draw)\n",
        "    # NEP I suspect that this is not a function, because result should be automatically decided, not called. But it might be if it's called every time you make a move.\n",
        "    # NEP updating: this should be written, because each time you run simulation you'd need to return a result (True/False), that can in return updates the nodes in BP.\n",
        "\n",
        "    # NEP it seems I need to toggle the player a bit here. But I have not figured things out, yet.\n",
        "    rows = (any(np.sum(self.states[player],axis=0))==3)  # NEP I am not too sure whether \"any\" would work fine here bc it's three rows to be \"any-ed\"\n",
        "    columns = (any(np.sum(self.states[player],axis=1))==3)\n",
        "    diagonals = (any(np.trace(self.states[player]))==3)\n",
        "    reverse_diagonals = (any(np.trace(np.flip(self.states[player], axis=1)))==3)\n",
        "    # NEP I felt confused whether I should have self.player or just player here.\n",
        "    self.reward =  rows or columns or digonals or reverse_diagonals  # if any suffice, we would return a True, which will send the signal back to the nodes in BP\n",
        "    return self.reward\n",
        "'''\n",
        "Confusions:\n",
        "- Transform natural language into game commands\n",
        "'''"
      ],
      "metadata": {
        "id": "RoMqnAerBaiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Node class --> A kind of treenode tailored for TTT game\n",
        "class Node():\n",
        "  def __init__(self, parent = None, children=[], board= None, idx = None):\n",
        "    self.reward = reward # reward associated to each node\n",
        "    self.children = children\n",
        "    self.visit = 0\n",
        "    self.win = 0\n",
        "    self.board = board\n",
        "    self.idx = idx\n",
        "    self.untravelled = board.untravelled # a list of indices pointing to untravelled nodes.\n",
        "    # NEP I think in extension/simulation/a copy of current board state, untravelled=unvisited. Bc in that simulation, you'd actually make moves on board state as you would with actual board.\n",
        "  # make a move  Conditions: highest UCB1 score\n",
        "  def select(self):\n",
        "    s = sorted(self.children, lambda c: c.wins/c.bisits + 0.2*np.sqrt(2*np.log(self.visits)/c.visits)) # NEP to verify UCB1 function\n",
        "    return s[-1]  # the last of an ascending list, the next move of largest return.\n",
        "\n",
        "  # connect a child to parent node. Input: current board, idx of untravelled node; Output: a connected child\n",
        "  def extend(self, board):  # NEP might still be wrong (input)\n",
        "    idx = np.random.choice(board.untravelled)\n",
        "    child = Node(parent=self, children=[], board=board, idx=idx) # NEP to modify parent and board. They should be special to current child.\n",
        "    node.untravelled.remove(idx)  # remove that from untravelled list.\n",
        "    self.children.append(child) # Successfully extended the node into the list of children (to be selected)\n",
        "\n",
        "\n",
        "  # for each node, there is win/visit num associated.\n",
        "  def update(self):\n",
        "    self.visit += 1   # each time you visit this node in simulation, you should call this update method.\n",
        "    if self.board.resulting():  # when such a path results in a 3.\n",
        "      self.win +=1\n",
        "\n"
      ],
      "metadata": {
        "id": "bgHFDkTAEdCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "rootstate = None # NEP This should be a user-defined current board state,which is a modified instance of Board().\n",
        "# MCTS # Specialized in MCTS strategy, not the game rules.\n",
        "max_iter = 100\n",
        "\n",
        "def MCTS(board = rootstate.copy(), max_iter = None):\n",
        "  root = Node(board=rootstate)  # current actor(node)\n",
        "  iter = 0\n",
        "\n",
        "  # each round is a simulation based on current board state/node\n",
        "  while iter <= max_iter and board.untravelled:\n",
        "    iter += 1\n",
        "    copy = board.copy()\n",
        "    node = root # bc you will traverse to this copy of board state\n",
        "    player = copy.player  # When updating board state, the players will take turn.\n",
        "\n",
        "    # Step 1 Selection: make a move / select a child in a simulation.\n",
        "    if copy.untravelled == [] and node.children != []: # Conditions: when the node is fully expanded and we have not reached terminal.\n",
        "      node = node.select()  # You traserse to best child on this copy state.\n",
        "\n",
        "    # Step 2 Expansion: the action of connecting to a untravelled node. Conditions: as long as there is untravelled position left\n",
        "    while copy.untravelled != []:\n",
        "      node = node.expand() # It seems you should include idx for that node here, but my method does not include such argument.\n",
        "      board.board_updating(states=copy, player=play, position=idx)  # where is idx again?\n",
        "\n",
        "    # Step 3 Simulation: Going down from the newly expanded node to the terminal\n",
        "    while copy.untravelled !=[] and not node.resulting():  # NEP 条件可能错了，看起来重复。\n",
        "      board.board_updating(states=copy, player=play, position=idx)  # 这里应该是expansion 之后的board states. But I'd assume it's updated by the step2 already.\n",
        "\n",
        "    # Step 4 Backpropagation: return the stats to the parent node (how many steps and how many wins)\n",
        "    while node:   # as long as there is a (parent) node we can come back to.\n",
        "      node.update()\n",
        "      node = node.parent\n",
        "\n",
        "    best_move = sorted(node.children, key=lambda c: c.wins/c.visits)\n",
        "  return  best_move # the move with highest score.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TsOKSrAjGT9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Annotation\n",
        "https://stackoverflow.com/questions/49456415/monte-carlo-tree-search-tic-tac-toe-poor-agent"
      ],
      "metadata": {
        "id": "LWyP3uX8Hi3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Board: # So in class Board, you want to define a function to return a copy (for simulation purpose), make the move that reflects  the change over the board, maintain a storage for untraveled nodes, and get result\n",
        "  '''\n",
        "  class handling state of the board\n",
        "  '''\n",
        "\n",
        "  def __init__(self):\n",
        "    self.state = np.zeros([2,3,3])  # 忘记了初始要用np.zeros\n",
        "    self.player = 0 # current player's turn  # remember thay they use  0 & 1 as a switcher for players to take turn. Cleaver. You can unify everything else accordingly. board states and positions of nodes can be just in one array.\n",
        "\n",
        "  def copy(self):   # You create a copy of current state/player on which you may run a simulation. It's like an instance of board, but with player/state info maintained.\n",
        "    '''\n",
        "    make copy of the board\n",
        "    '''\n",
        "    copy = Board()\n",
        "    copy.player = self.player\n",
        "    copy.state = np.copy(self.state)\n",
        "    return copy\n",
        "\n",
        "  '''\n",
        "  def move(self, move):  # move para better be \"position\", of [x,y] shape\n",
        "\n",
        "    take move of form [x,y] and play\n",
        "    the move for the current player\n",
        "    if np.any(self.state[:,move[0],move[1]]): return  # if occupied, then you don't occupy it again.\n",
        "    self.state[self.player][move[0],move[1]] = 1  # NEP smart. I was confused how can they find a position to modify. This is how.\n",
        "    self.player ^= 1   # togging betw two states; XOR operation. Clever.\n",
        "  '''\n",
        "\n",
        "# modifying board state: from an idx to a changed board state.\n",
        "def make_move(self, position):  # position: of [x,y] shape\n",
        "    '''\n",
        "    Places a marker at the specified board position [row, col]\n",
        "    for the current player if the position is empty\n",
        "    '''\n",
        "    row, col = position\n",
        "    if np.any(self.state[:, row, col]):  # if cell is occupied\n",
        "        return  # position already taken\n",
        "\n",
        "    self.state[self.player][row, col] = 1  # NEP smart. I was confused how can they find a position to modify. This is how.\n",
        "    self.player ^= 1  # togging betw two states; XOR operation. Clever.\n",
        "\n",
        "\n",
        "# get remaining untravelled locations.\n",
        "def get_moves(self):  # Seems to miss the scenarios where game is won and over.\n",
        "  '''\n",
        "  return remaining possible board moves\n",
        "  (ie where there are no O's or X's)\n",
        "  '''\n",
        "  if self.check_for_win():\n",
        "    return []  # ensure that nobody has won yet\n",
        "  return np.argwhere(self.state[0]+self.state[1]==0).tolist()  # np.argwhere: Find the indices of array elements that are non-zero (True values in a boolean array), grouped by element.\n",
        "  # NEP it will return a list of indices (x,y) indicating positions both empty.\n",
        "\n",
        "  '''\n",
        "\n",
        "  def result(self):\n",
        "\n",
        "    check rows, columns, and diagonals\n",
        "    for sequence of 3 X's or 3 O's\n",
        "    board = self.state[self.player^1]  # This guy just uses a load of confusing naming. Cannot you fucking just have a new name?\n",
        "    col_sum = np.any(np.sum(board,axis=0)==3)\n",
        "    row_sum = np.any(np.sum(board,axis=1)==3)\n",
        "    d1_sum  = np.any(np.trace(board)==3)  # Return the sum along diagonals of the array.\n",
        "    d2_sum  = np.any(np.trace(np.flip(board,1))==3) # Flip an array horizontally (axis=1).\n",
        "    return col_sum or row_sum or d1_sum or d2_sum  # Return a True if any of those is true otherwise False.\n",
        "    '''\n",
        "\n",
        "def check_for_win(self):\n",
        "    previous_player_board = self.state[self.player^1]  # or last_move_board\n",
        "\n",
        "    has_winning_column = np.any(np.sum(previous_player_board, axis=0) == 3)\n",
        "    has_winning_row = np.any(np.sum(previous_player_board, axis=1) == 3)\n",
        "    has_winning_diagonal = np.any(np.trace(previous_player_board) == 3)\n",
        "    has_winning_antidiagonal = np.any(np.trace(np.flip(previous_player_board, 1)) == 3)\n",
        "\n",
        "    return has_winning_column or has_winning_row or has_winning_diagonal or has_winning_antidiagonal"
      ],
      "metadata": {
        "id": "odwAOY9FLH7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Annotating other's code\n",
        "\n",
        "class Node: # For a class Node, you'd want its parent (for returning stats), its children (for deciding best node), maintaining/updating wins & visits, act to select best child, expand to an untraveled node,\n",
        "  '''\n",
        "  maintains state of nodes in\n",
        "  the monte carlo search tree\n",
        "  '''\n",
        "  def __init__(self, parent=None, idx_move=None, board=None):  # I do not know what is an action\n",
        "    self.parent = parent  # NEP why. It's just current node. It turns out that TreeNode just defines a node as None, but with attributes and operations within __init__, and ofc methods.\n",
        "    self.board = board\n",
        "    self.children = []  # adding a new child node (default, but also based on currect action & board)\n",
        "    self.wins = 0\n",
        "    self.visits = 0\n",
        "    self.untried_moves = board.get_moves()  # list of indices pointing to untraveled nodes\n",
        "    # Note though, node.untried moves != board.get_moves().\n",
        "    # The former is updated when you expand to a new node in simulation\n",
        "    # whereas the latter remains faithful to untravelled positions on current board state.\n",
        "\n",
        "    self.idx_move = idx_move # I hate his code! But this should be an idx pointing to a position (since it can be removed from list of untravaled_nodes)\n",
        "\n",
        "  def select(self):\n",
        "    '''\n",
        "    select child of node with\n",
        "    highest UCB1 value\n",
        "    '''\n",
        "    s = sorted(self.children, key=lambda c:c.wins/c.visits+0.2*sqrt(2*log(self.visits)/c.visits))\n",
        "    return s[-1] # by default, last is the largest. So select will choose a move that has the most return among all children of a node.\n",
        "\n",
        "  def expand(self, idx_move, board): # here you pass on a seperate board state , idx_move because you want to pass on them to define the child node.\n",
        "    '''\n",
        "    expand parent node (self) by adding child\n",
        "    node with given action and state\n",
        "    '''\n",
        "    child = Node(parent=self, idx_move=idx_move, board=board)  # NEP Interesting. This was one of my mental blocks. It turns out that you can reference a Node from within.\n",
        "    self.untried_moves.remove(idx_move)\n",
        "    self.children.append(child)\n",
        "    return child\n",
        "\n",
        "  def update(self, result):\n",
        "    self.visits += 1\n",
        "    self.wins += result"
      ],
      "metadata": {
        "id": "ABPwZlZjHn2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SO MCTS algo is a function, just like a remote cheating super computer, which, based on current board state, once you click, return you a move rec.\n",
        "def UCT(rootstate, maxiters):  # to give a rec at a given place, you run mixiters of simulation atmost.\n",
        "\n",
        "  root = Node(board=rootstate) # I think the rootstate is an instance that refers to a board state where you've made all actual moves until now.\n",
        "\n",
        "  for i in range(maxiters):   # NEP case is now clear. You just need to create a whole board state each round of simulation.\n",
        "    node = root\n",
        "    board = rootstate.copy() # and for each board state on which you wanted to make simulations,  you make # of simulations & board states.\n",
        "\n",
        "    # selection (exploitation) - select best child if parent fully expanded and not terminal\n",
        "    # NEP i have not gotten the while condition here.\n",
        "    while node.untried_moves == [] and node.children != []:\n",
        "      node = node.select()\n",
        "      board.make_move(node.idx_move)  # NEP I cannot see where they got this node.idx_move from. Possible it's just from children, which is from rootstateboard.\n",
        "\n",
        "\n",
        "    # another mental bottleneck is whether it's ok to expand the board for just once as it is done here.\n",
        "    # expansion (exploration) - expand parent to a random untried action, modify that board position, and connect parent to this new child\n",
        "    if node.untried_moves != []:  # as long as there are untried moves in this simulation you can expand it once\n",
        "      a = random.choice(node.untried_actions)  # one item is an idx of a random move\n",
        "      board.move(a)\n",
        "      node = node.expand(a, board.copy())\n",
        "      # NEP when you EXPAND, you make a random untraveled move, you mark that move in this iteration of board copy, and add that node in current node's children list.\n",
        "\n",
        "    # simulation - rollout to terminal state from current\n",
        "    # state using random actions\n",
        "    while board.get_moves() != [] and not board.result():  # as long as there are still unoccupied positions you can move this copy\n",
        "      board.move(random.choice(board.get_moves()))\n",
        "      # NEP: in simulation you roll each board state + path to its terminal (meaning you get the final result)\n",
        "\n",
        "    # backpropagation - propagate result of rollout game up the tree\n",
        "    # reverse the result if player at the node lost the rollout game\n",
        "    '''\n",
        "    while node != None:\n",
        "      result = board.result()\n",
        "      if result:\n",
        "        if node.board.player==board.player:\n",
        "          result = 1  # Fuck you. Use a different name.\n",
        "        else: result = -1 # current player loses the game here.\n",
        "      else: result = 0\n",
        "      node.update(result)\n",
        "      '''\n",
        "\n",
        "    while node is not None:\n",
        "        has_won = board.result()  # or is_game_won, has_winning_line\n",
        "        if has_won:\n",
        "            if node.board.player == board.player:\n",
        "                score = 1    # current player wins\n",
        "            else:\n",
        "                score = -1   # current player loses\n",
        "        else:\n",
        "            score = 0       # game continues/draw\n",
        "        node.update(score)\n",
        "      node = node.parent # each time you return to the parent node until None.\n",
        "      # NEP for a current board (some traveled some untraveled), you update each node whether it would win the game or not (result=1,0,-1) if continued\n",
        "  s = sorted(root.children, key=lambda c:c.wins/c.visits)\n",
        "  return s[-1].action   # So it seems each time you run a UCT, which is a MCTS-based strategy and return rec of next move, instead of playing the whole game as I thought."
      ],
      "metadata": {
        "id": "Abk6pw37QqoS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}