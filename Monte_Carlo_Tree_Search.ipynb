{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Oct 18 - Work in progress\n",
        "References:\n",
        "https://int8.io/monte-carlo-tree-search-beginners-guide/\n",
        "https://www.geeksforgeeks.org/ml-monte-carlo-tree-search-mcts/\n",
        "https://www.reddit.com/r/singularity/comments/1cmcmy5/monte_monte_carlo_tree_search_with_llms_is_the/\n",
        "Search: Games, Minimax, and Alpha-Beta: https://ocw.mit.edu/courses/6-034-artificial-intelligence-fall-2010/resources/lecture-6-search-games-minimax-and-alpha-beta/\n",
        "https://courses.cs.washington.edu/courses/cse599i/18wi/resources/lecture19/lecture19.pdf\n",
        "applications illustrated: https://www.informs-sim.org/wsc18papers/includes/files/021.pdf\n",
        "tic-tak-toe: https://stackoverflow.com/questions/49456415/monte-carlo-tree-search-tic-tac-toe-poor-agent\n"
      ],
      "metadata": {
        "id": "yLGOO2DqyQDp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MCTS\n",
        "What is it for?\n",
        "- To create a tree structure search algorithm that balances depth and breadth (exploitation and exploration)\n",
        "\n",
        "\n",
        "How it works?\n",
        "- Each node should be a value the agents get by traversing to that node, whatever value that can be.\n",
        "- When traveling to each node, you simulate the expected returns of current path and compare it with rewards of best strategy so far.\n",
        "- You should probably estimate current resources, too.\n",
        "\n",
        "- The resources and rewards may or may not be the same thing. Likely you may gain resources as rewards, but very likely they are different things.\n",
        "- The final result is to pick up a path with largest expected return, before the resources run out, or you use as little resources as possible.\n",
        "\n",
        "Selection: the act of choosing a path based on current informaion (resources/rewards). My understanding is it start with a random strategy until you get to know a better one.\n",
        "\n",
        "Expansion: go down to the end of current path\n",
        "\n",
        "Simulation: est rewards based on expected return of current path\n",
        "\n",
        "Backpropagation: ????\n",
        "\n",
        "\n",
        "Where things may get tricky?\n",
        "- It seems MCTS is computationally demanding\n",
        "-\n",
        "\n",
        "Why does it even work in many applications?\n",
        "-\n",
        "- Computation would not be a limitation for employijng such strategy in reality. In other words, simulation and computation enable calculaton of best strategy. <br><br><br>\n",
        "\n",
        "\n",
        "The main concept of monte carlo tree search is a search. Search is a set of traversals down the game tree. Single traversal is a path from a root node (current game state) to a node that is not fully expanded. Node being not-fully expanded means at least one of its children is unvisited, not explored. Once not fully expanded node is encountered, one of its unvisited children is chosen to become a root node for a single playout/simulation. The result of the simulation is then propagated back up to the current tree root updating game tree nodes statistics. Once the search (constrained by time or computational power) terminates, the move is chosen based on the gathered statistics.\n",
        "\n",
        "Lots of dots not clearly connected together, right?\n",
        "\n",
        "Let’s try to ask crucial questions regarding the simplified description above to slowly understand all the pieces:\n",
        "\n",
        "what are expanded or not fully unexpanded game tree nodes?\n",
        "what it means ‘to traverse down‘ during search? How is the next (child) node selected?\n",
        "what is a simulation?\n",
        "what is the backpropagation?\n",
        "what statistics are back-propagated and updated in expanded game tree nodes ?\n",
        "How is the final move even chosen ?"
      ],
      "metadata": {
        "id": "Rz3MGzzootrP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7mVNk2-lqhi"
      },
      "outputs": [],
      "source": [
        "# Pseudocode for MCTS\n",
        "class MCTS():\n",
        "\n",
        "  # global variables\n",
        "  def __init__(self,):\n",
        "    self.resources  = 100\n",
        "    self.rewards = [all_reward, reward_pathA, reward_pathB, ...] # should be a dict\n",
        "    self.steps = [all_steps, step_pathA, step_pathB, ...] # should be a dict\n",
        "    self.best_path = {} # best path in our knowledge. dict, node:reward\n",
        "\n",
        "  def resource_monitor(self,):   # it's called every time the agent makes a move.\n",
        "    if self.resources ==0:\n",
        "      return \"Game Over\"\n",
        "    self.resource -= 1   # there is no return so far, but when you call this global variable it always tells you current remain resources.\n",
        "\n",
        "  def policy(self,)  # the method to decide next move. It starts with a random strategy, and then moves based on our calculation of best path\n",
        "    reward_new_path = simulation(path, reward)\n",
        "    reward_best_path = list(self.best_path.values).sum()\n",
        "    if ward_new_path > reward_best_path:\n",
        "      self.best_path = new_path # where you got this new path?\n",
        "    return self.best_path\n",
        "\n",
        "  def stats(self, ):\n",
        "    expected_rewards\n",
        "    expected_resouces\n",
        "\n",
        "  def simultation(self,path, reward): # it's called when you need to cal expected rewards of a given path\n",
        "    expected_reward = self.rewads[path].sum().self.steps[path] * self.resources # assume resources are measured in the same way as steps, how much more rewards you may get in a given path with the resources given.\n",
        "    return expected_rewards\n",
        "\n",
        "\n",
        "  def actions_in_node(self, node):\n",
        "    self.resource_monitor()\n",
        "    updating_best_path(path, reward)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}