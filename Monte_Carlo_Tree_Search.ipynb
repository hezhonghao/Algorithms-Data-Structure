{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Oct 18 - Work in progress <br>\n",
        "References:<br>\n",
        "https://int8.io/monte-carlo-tree-search-beginners-guide/<br>\n",
        "https://www.geeksforgeeks.org/ml-monte-carlo-tree-search-mcts/<br>\n",
        "https://www.reddit.com/r/singularity/comments/1cmcmy5/monte_monte_carlo_tree_search_with_llms_is_the/ <br>\n",
        "Search: Games, Minimax, and Alpha-Beta: https://ocw.mit.edu/courses/6-034-artificial-intelligence-fall-2010/resources/lecture-6-search-games-minimax-and-alpha-beta/\n",
        "https://courses.cs.washington.edu/courses/cse599i/18wi/resources/lecture19/lecture19.pdf<br>\n",
        "applications illustrated: https://www.informs-sim.org/wsc18papers/includes/files/021.pdf<br>\n",
        "tic-tak-toe: https://stackoverflow.com/questions/49456415/monte-carlo-tree-search-tic-tac-toe-poor-agent <br>\n",
        "\n",
        "some leetcode adaptations: https://www.perplexity.ai/search/any-problems-at-leetcode-that-OgI491GFTbKDPRnDknJCUg\n"
      ],
      "metadata": {
        "id": "yLGOO2DqyQDp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MCTS\n",
        "What is it for?\n",
        "- To create a tree structure search algorithm that balances depth and breadth (exploitation and exploration)\n",
        "\n",
        "\n",
        "How it works?\n",
        "- Each node should be a value the agents get by traversing to that node, whatever value that can be.\n",
        "- When traveling to each node, you simulate the expected returns of current path and compare it with rewards of best strategy so far.\n",
        "- You should probably estimate current resources, too.\n",
        "\n",
        "- The resources and rewards may or may not be the same thing. Likely you may gain resources as rewards, but very likely they are different things.\n",
        "- The final result is to pick up a path with largest expected return, before the resources run out, or you use as little resources as possible.\n",
        "\n",
        "Selection: the act of choosing a path based on current informaion (resources/rewards). My understanding is it start with a random strategy until you get to know a better one.\n",
        "\n",
        "Expansion: go down to the end of current path\n",
        "\n",
        "Simulation: est rewards based on expected return of current path\n",
        "\n",
        "Backpropagation: ????\n",
        "\n",
        "\n",
        "Where things may get tricky?\n",
        "- It seems MCTS is computationally demanding\n",
        "-\n",
        "\n",
        "Why does it even work in many applications?\n",
        "-\n",
        "- Computation would not be a limitation for employijng such strategy in reality. In other words, simulation and computation enable calculaton of best strategy. <br><br><br>\n",
        "\n",
        "\n",
        "The main concept of monte carlo tree search is a search. Search is a set of traversals down the game tree. Single traversal is a path from a root node (current game state) to a node that is not fully expanded. Node being not-fully expanded means at least one of its children is unvisited, not explored. Once not fully expanded node is encountered, one of its unvisited children is chosen to become a root node for a single playout/simulation. The result of the simulation is then propagated back up to the current tree root updating game tree nodes statistics. Once the search (constrained by time or computational power) terminates, the move is chosen based on the gathered statistics.\n",
        "\n",
        "Lots of dots not clearly connected together, right?\n",
        "\n",
        "Let’s try to ask crucial questions regarding the simplified description above to slowly understand all the pieces:\n",
        "\n",
        "what are expanded or not fully unexpanded game tree nodes?\n",
        "what it means ‘to traverse down‘ during search? How is the next (child) node selected?\n",
        "what is a simulation?\n",
        "what is the backpropagation?\n",
        "what statistics are back-propagated and updated in expanded game tree nodes ?\n",
        "How is the final move even chosen ?"
      ],
      "metadata": {
        "id": "Rz3MGzzootrP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "B7mVNk2-lqhi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "collapsed": true,
        "outputId": "7c940c89-4fff-4583-d856-e7c6d69d7f21"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "expected ':' (<ipython-input-1-6566e6b25141>, line 12)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-6566e6b25141>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    def forward(self, resources, node)\u001b[0m\n\u001b[0m                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected ':'\n"
          ]
        }
      ],
      "source": [
        "# Pseudocode for MCTS (not using a class bc self. not clean )\n",
        "resources = None\n",
        "\n",
        "# overall function\n",
        "def forward(resources, node):\n",
        "  while resources:\n",
        "\n",
        "\n",
        "# Step 1 Selection: the process of deciding next move(node) based on expected rewards\n",
        "def selection(nodes, rewards):   # I expect nodes to be a tree structure but rewards to be a dict: node:reward\n",
        "  best_child = best(nodes)  # basically a for loop to go through all nodes and rewards I think\n",
        "  return best_child\n",
        "\n",
        "# Step 2 Expansion: the action of creating a node/making a move\n",
        "def expansion(nodes):\n",
        "  if resources == 0:\n",
        "    return None\n",
        "  cur.next = best_child\n",
        "  cur = cur.next\n",
        "  return cur # # NEP I could not really differentiate this from the selection\n",
        "\n",
        "# Step 3 Simulation: to calculating expected rewards if you were to exploit current node/path\n",
        "def simulation(node, rewards):  # Assuming that this is to cal expected return of the current path if you exploit it.\n",
        "\n",
        "  return simulated_rewards\n",
        "\n",
        "# Step 4 Backpropagation: return the stats to the parent node (how many nodes and how much expected rewards)\n",
        "def backpropagation():\n",
        "  return stats # signals back to the parents  dict(node:reward)\n",
        "\n",
        "\n",
        "\n",
        "# Helper functions\n",
        "\n",
        "# Strategies for expansion\n",
        "def strategies(nodes):\n",
        "  # default   # we start with a random strategy\n",
        "  pick_randome(node.children)\n",
        "\n",
        "  # best strategy (???)\n",
        "  reward_new_path = simulation(path, reward)\n",
        "  reward_best_path = list(self.best_path.values).sum()\n",
        "  if ward_new_path > reward_best_path:\n",
        "    self.best_path = new_path # where you got this new path?\n",
        "  return best_path\n",
        "\n",
        "# Monitoring steps and rewards\n",
        "\n",
        "# Monitoring resources\n",
        "def resource_monitor():   # it's called every time the agent makes a move.\n",
        "  if resources ==0:\n",
        "    return \"Game Over\"\n",
        "  resource -= 1   # there is no return so far, but when you call this global variable it always tells you current remain resources.\n",
        "\n",
        "# Monitoring rewards\n",
        "def rewards monitor():\n",
        "\n",
        "  # current rewards\n",
        "  dict(node:rewards)\n",
        "\n",
        "  # expecter rewards based on simulations\n",
        "  dict(node: expected_rewards)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A pseudocode from https://int8.io/monte-carlo-tree-search-beginners-guide/, with annotation\n",
        "# Overall function: selection - expansion - simulation - backpropagation - selection, until a optimal path is found.\n",
        "def monte_carlo_tree_search(root):\n",
        "    while resources_left(time, computational_power):  # Continue the process until resources (time or computational power) are exhausted\n",
        "        leaf = traverse(root)  # Step 1: Selection - traverse the tree to select a leaf node (unexplored node)\n",
        "        simulation_result = rollout(leaf)  # Step 2: Simulation - simulate a random play from the leaf node to a terminal state\n",
        "        backpropagate(leaf, simulation_result)  # Step 3: Backpropagation - update the stats of all the nodes from the leaf back to the root\n",
        "    return best_child(root)  # Step 4: Return the best child (the most promising move) of the root based on the number of visits or score\n",
        "    # NEP is best_child(root) a path or a node\n",
        "# Step 1: Selection - traverse the tree to select a leaf node (unexplored node)\n",
        "def traverse(node):\n",
        "    while fully_expanded(node):  # Keep traversing the tree as long as the node is fully expanded (all children have been visited)\n",
        "        node = best_uct(node)  # Choose the child with the best UCT (Upper Confidence bound applied to Trees) score for exploration\n",
        "    return pick_unvisited(node.children) or node  # If there are unvisited children, return one. Otherwise, return the current node (could be terminal)\n",
        "    #NEP what is UCT and do we need to write a UCT function in real code?  # NEP Do we have a function to monitor unvisited node?\n",
        "\n",
        "# Step 2: Expansion (make a move in real games)\n",
        "def expand(node):\n",
        "    if not fully_expanded(node):\n",
        "        return pick_unvisited(node.children)  # Expand by adding a new unvisited node\n",
        "    return node  # If already fully expanded, return the current node\n",
        "\n",
        "# Step 3: Simulation - simulate a random game (play) from the node until a terminal state is reached\n",
        "def rollout(node):\n",
        "    while non_terminal(node):  # Keep going until you reach a terminal node (end of the game)\n",
        "        node = rollout_policy(node)  # Apply a rollout policy (typically random) to pick the next move/child\n",
        "    return result(node)  # Return the result of the game (win, loss, or draw)\n",
        "\n",
        "# strategy used in simulation\n",
        "def rollout_policy(node):  # A simple strategy for random simulation, picks a random child (move) from the current node\n",
        "    return pick_random(node.children)\n",
        "\n",
        "# Step 3: Backpropagation - propagate the result of the simulation back up the tree\n",
        "def backpropagate(node, result):\n",
        "   if is_root(node):  # Stop when we reach the root of the tree\n",
        "       return\n",
        "   node.stats = update_stats(node, result)  # Update the node's stats (e.g., number of visits, win/loss score) based on the result\n",
        "   backpropagate(node.parent)  # Recursively backpropagate the result to the parent node\n",
        "   # NEP are results & stats the same thing?\n",
        "def best_child(node):  # Step 4: Selection - choose the child with the best stats (e.g., highest visit count or average win rate)\n",
        "    return pick_child_with_highest_visits(node.children)  # Usually, this is based on the number of visits or average reward (score)\n",
        "    # NEP I didn't cound whether you would decide best child according to visit count."
      ],
      "metadata": {
        "id": "8haO3Yc6f_Uq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}